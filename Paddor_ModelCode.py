# -*- coding: utf-8 -*-
"""ML Individual Assignment Pipeline

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1YXQHctqpWaBtLaaQZlovFwoTnfYwHtnB
"""

import numpy as np
import pandas as pd
from sklearn.model_selection import train_test_split
from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer
from sklearn.model_selection import cross_val_score
from sklearn.svm import LinearSVC
from sklearn.metrics import accuracy_score
from sklearn.pipeline import Pipeline
from sklearn.metrics import f1_score
import pickle

df = pd.read_csv("train (1).csv")
df2 = df.copy()

#EDA
df.dtypes
df.rating.hist()
# most ratings are 6 and above

def rating(rating):
    if rating == 1:
        return 'negative'
    elif rating == 2:
        return 'negative'
    elif rating == 3:
        return 'negative'
    elif rating == 4:
        return 'negative'  
    else:
        return 'positive'
df2.rating = df2.rating.apply(rating)

X = df2.loc[:, ['benefits_review','side_effects_review','comments_review']]
y = df2.rating
X, y = df2.comments_review.fillna(' '), df2.rating

X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3,stratify=y)

X_train_docs = [doc for doc in X_train]

import spacy
import re
from spacy.lang.en.stop_words import STOP_WORDS
en_nlp = spacy.load('en_core_web_sm', disable=['parser', 'ner'])
pattern = re.compile('(?u)\\b\\w\\w+\\b')

import spacy
nlp = spacy.load("en_core_web_sm")

"""#**using a spacy tokenizer similar to in-class**"""

def custom_tokenizer(document):
  doc_spacy = en_nlp(document)
  lemmas = [token.lemma_ for token in doc_spacy]
  pattern = re.compile('(?u)\\b\\w\\w+\\b')
  return [token for token in lemmas if token not in STOP_WORDS and pattern.match(token)]

pipeline = Pipeline([
  ('vect', TfidfVectorizer(tokenizer = custom_tokenizer )),
  ('cls',LinearSVC())
])

pipeline.fit(X_train_docs, y_train)

cross_val_score(pipeline, X_train_docs, y_train, cv=5).mean()

predicted= pipeline.predict([doc for doc in X_test])

accuracy_score(y_test, predicted)

f1_score(y_test, predicted, pos_label='positive')

df_sample = pd.read_csv("test (1).csv")

df_sample['review'] = df_sample['benefits_review'].astype(str) + df_sample['side_effects_review'].astype(str) + df_sample['comments_review'].astype(str)
df_sample.rating.shape

X_sample = df_sample.review

def rating(rating):
    if rating == 1:
        return 'negative'
    elif rating == 2:
        return 'negative'
    elif rating == 3:
        return 'negative'
    elif rating == 4:
        return 'negative'  
    else:
        return 'positive'
df_sample.rating = df_sample.rating.apply(rating)

y_sample= df_sample.rating
y_sample.shape

predicted_sample = pipeline.predict([doc for doc in X_sample])

accuracy_score(y_sample, predicted_sample)

f1_score(y_sample, predicted_sample, pos_label='positive')

"""##**Using a basic Spacy tokenizer**"""

def spacy_tokenizer(doc):
    tokens = nlp(doc)
    return [token.text for token in tokens if not token.is_stop and not token.is_punct and not token.is_space]

pipeline2 = Pipeline([
  ('vect', TfidfVectorizer(tokenizer = spacy_tokenizer)),
  ('cls',LinearSVC())
])

pipeline2.fit(X_train_docs, y_train)

cross_val_score(pipeline2, X_train_docs, y_train, cv=5).mean()

predicted2= pipeline2.predict([doc for doc in X_test])

accuracy_score(y_test, predicted2)

f1_score(y_test, predicted2, pos_label='positive')

predicted2_sample = pipeline2.predict([doc for doc in X_sample])

accuracy_score(y_sample, predicted2_sample)

f1_score(y_sample, predicted2_sample, pos_label = 'positive')

"""##**Using a spacy tokenizer with POS**"""

nlp = spacy.load("en_core_web_sm")

def custom_tokenizer2(document):
    doc_spacy = nlp(document)
    return [token.lemma_ for token in doc_spacy if token.pos_ in ['ADJ', 'NOUN']]

pipeline3 = Pipeline([
  ('vect', TfidfVectorizer(tokenizer = custom_tokenizer2)),
  ('cls',LinearSVC())
])

pipeline3.fit(X_train_docs, y_train)

cross_val_score(pipeline3, X_train_docs, y_train, cv=5).mean()

predicted3= pipeline3.predict([doc for doc in X_test])

accuracy_score(y_test, predicted3)

f1_score(y_test, predicted3, pos_label='positive')

predicted_sample3 = pipeline3.predict([doc for doc in X_sample])

accuracy_score(y_sample, predicted_sample3)

f1_score(y_sample, predicted_sample3, pos_label='positive')

"""##**Using both POS & STOP_words**


"""

def custom_tokenizer3(text):
    doc = nlp(text)
    return [token.lemma_ for token in doc if not token.is_stop and token.pos_ in ['NOUN', 'ADJ', 'VERB', 'ADV']]

pipeline4 = Pipeline([
  ('vect', TfidfVectorizer(tokenizer = custom_tokenizer3)),
  ('cls',LinearSVC())
])

pipeline4.fit(X_train_docs, y_train)

cross_val_score(pipeline4, X_train_docs, y_train, cv=5).mean()

predicted4 = pipeline4.predict([doc for doc in X_test])

accuracy_score(y_test, predicted4)

f1_score(y_test, predicted4, pos_label = 'positive')

predicted_sample4 = pipeline4.predict([doc for doc in X_sample])

accuracy_score(y_sample, predicted_sample4)

f1_score(y_sample, predicted_sample4, pos_label='positive')

"""##**Using lexicons, stop words, and POS**"""

!pip install afinn

from afinn import Afinn
afinn = Afinn()

def custom_tokenizer5(doc):
    doc = nlp(doc)
    tokens = [token.lemma_.lower() for token in doc if token.pos_ in ["ADJ", "VERB", "NOUN"] and token.lemma_.lower() not in STOP_WORDS]
    sentiment = afinn.score(" ".join(tokens))
    return tokens

pipeline5 = Pipeline([
  ('vect', TfidfVectorizer(tokenizer = custom_tokenizer5)),
  ('cls',LinearSVC())
])

pipeline5.fit(X_train_docs, y_train)

cross_val_score(pipeline5, X_train_docs, y_train, cv=5).mean()

predicted5 = pipeline5.predict([doc for doc in X_test])

accuracy_score(y_test, predicted5)

f1_score(y_test, predicted5, pos_label = 'positive')

predicted_sample5 = pipeline5.predict([doc for doc in X_sample])

accuracy_score(y_sample, predicted_sample5)

f1_score(y_sample, predicted_sample5, pos_label='positive')

"""##**Transforming Pipeline2 into a pickle file for Streamlit**"""

# assume pipeline is defined and trained
with open('ML_Streamlit_Pipeline.pkl', 'wb') as f:
    pickle.dump(pipeline2, f)